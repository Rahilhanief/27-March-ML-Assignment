{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e11802-61cb-44e8-81d0-77958385e5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model\\nthat determines the proportion of variance in the dependent variable that can be explained by the independent variable.\\nIn other words, r-squared shows how well the data fit the regression model (the goodness of fit).\\n\\nR 2 = 1 − sum squared regression (SSR) total sum of squares (SST) , = 1 − ∑ ( y i − y i ^ ) 2 ∑ ( y i − y ¯ ) 2 .\\nThe sum squared regression is the sum of the residuals squared, \\nand the total sum of squares is the sum of the distance the data is away from the mean all squared.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 1:\n",
    "\"\"\"R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model\n",
    "that determines the proportion of variance in the dependent variable that can be explained by the independent variable.\n",
    "In other words, r-squared shows how well the data fit the regression model (the goodness of fit).\n",
    "\n",
    "R 2 = 1 − sum squared regression (SSR) /total sum of squares (SST) , = 1 − ∑ ( y i − y i ^ ) 2/ ∑ ( y i − y ¯ ) 2 .\n",
    "The sum squared regression is the sum of the residuals squared, \n",
    "and the total sum of squares is the sum of the distance the data is away from the mean all squared.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac75151-60c1-4b77-920e-60102dc4dadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model.\\nThe adjusted R-squared increases when the new term improves the model more than would be expected by chance.\\nIt decreases when a predictor improves the model by less than expected.\\nThe difference between R squared and adjusted R squared value is that R squared value assumes that all the independent\\nvariables considered affect the result of the model, whereas the adjusted R squared\\nvalue considers only those independent variables which actually have an effect on the performance of the model.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 2:\n",
    "\"\"\"Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model.\n",
    "The adjusted R-squared increases when the new term improves the model more than would be expected by chance.\n",
    "It decreases when a predictor improves the model by less than expected.\n",
    "The difference between R squared and adjusted R squared value is that R squared value assumes that all the independent\n",
    "variables considered affect the result of the model, whereas the adjusted R squared\n",
    "value considers only those independent variables which actually have an effect on the performance of the model.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ba747a-3a21-45e4-85f8-9ac72f0e5204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clearly, it is better to use Adjusted R-squared when there are multiple variables in the regression model.\\nThis would allow us to compare models with differing numbers of independent variables.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 3 :\n",
    "\"\"\"Clearly, it is better to use Adjusted R-squared when there are multiple variables in the regression model.\n",
    "This would allow us to compare models with differing numbers of independent variables.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc96377f-af86-4fcf-8ade-8faa8768586f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Mean absolute error represents the average of the absolute difference between the actual \\nand predicted values in the dataset. It measures the average of the residuals in the dataset.\\nMean Squared Error represents the average of the squared difference between the original and\\npredicted values in the data set. It measures the variance of the residuals.\\nRoot Mean Squared Error is the square root of Mean Squared error. It measures the standard deviation of residuals.\\n\\nfrom sklearn.metrics import mean_squared_error\\nfrom sklearn.metrics import mean_absolute_error\\nmse=mean_squared_error(y_test,y_pred_test)\\nmae=mean_absolute_error(y_test,y_pred_test)\\nrmse=np.sqrt(mse)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 4 :\n",
    "\"\"\"The Mean absolute error represents the average of the absolute difference between the actual \n",
    "and predicted values in the dataset. It measures the average of the residuals in the dataset.\n",
    "Mean Squared Error represents the average of the squared difference between the original and\n",
    "predicted values in the data set. It measures the variance of the residuals.\n",
    "Root Mean Squared Error is the square root of Mean Squared error. It measures the standard deviation of residuals.\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mse=mean_squared_error(y_test,y_pred_test)\n",
    "mae=mean_absolute_error(y_test,y_pred_test)\n",
    "rmse=np.sqrt(mse)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4acd4367-0913-43c2-b842-206c8618a459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1.MSE:\\nAdvantage: \\n    The MSE is great for ensuring that our trained model has no outlier predictions with huge errors, \\n    since the MSE puts larger weight on theses errors due to the squaring part of the function. \\nDisadvantage: \\n    If our model makes a single very bad prediction, the squaring part of the function magnifies the error.\\n2.MAE:\\nAdvatanges:\\n    It is an easy to calculate evaluation metric.\\n    All the errors are weighted on the same scale since absolute values are taken.\\n    It is useful if the training data has outliers as MAE does not penalize high errors caused by outliers.\\n    It provides an even measure of how well the model is performing.\\nDisadvantage:\\n    Sometimes the large errors coming from the outliers end up being treated as the same as low errors.\\n    One of the main disadvantages of MAE is that it is not differentiable at zero.\\n    Many optimization algorithms tend to use differentiation to find the optimum value for parameters in the \\n    evaluation metric.\\n    It can be challenging to compute gradients in MAE.\\nRMSE :\\nAdvantages :\\n    RMSE is easy to understand.\\n    It serves as a heuristic for training models.\\nDisadvantages:\\n    One major drawback of RMSE is its sensitivity to outliers\\n    and the outliers have to be removed for it to function properly.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 5 :\n",
    "\"\"\"\n",
    "1.MSE:\n",
    "Advantage: \n",
    "    The MSE is great for ensuring that our trained model has no outlier predictions with huge errors, \n",
    "    since the MSE puts larger weight on theses errors due to the squaring part of the function. \n",
    "Disadvantage: \n",
    "    If our model makes a single very bad prediction, the squaring part of the function magnifies the error.\n",
    "2.MAE:\n",
    "Advatanges:\n",
    "    It is an easy to calculate evaluation metric.\n",
    "    All the errors are weighted on the same scale since absolute values are taken.\n",
    "    It is useful if the training data has outliers as MAE does not penalize high errors caused by outliers.\n",
    "    It provides an even measure of how well the model is performing.\n",
    "Disadvantage:\n",
    "    Sometimes the large errors coming from the outliers end up being treated as the same as low errors.\n",
    "    One of the main disadvantages of MAE is that it is not differentiable at zero.\n",
    "    Many optimization algorithms tend to use differentiation to find the optimum value for parameters in the \n",
    "    evaluation metric.\n",
    "    It can be challenging to compute gradients in MAE.\n",
    "RMSE :\n",
    "Advantages :\n",
    "    RMSE is easy to understand.\n",
    "    It serves as a heuristic for training models.\n",
    "Disadvantages:\n",
    "    One major drawback of RMSE is its sensitivity to outliers\n",
    "    and the outliers have to be removed for it to function properly.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24fa7a49-c51c-4e68-a815-0103ebb96c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lasso regression is used for eliminating automated variables and the selection of features.\\nThis method is usually used in machine learning for the selection of the subset of variables.\\nIt provides greater prediction accuracy as compared to other regression models.\\nLasso Regularization helps to increase model interpretation.\\nThe less important features of a dataset are penalized by the lasso regression.\\nRidge regression is faster compared to lasso but then again lasso has the advantage\\nof completely reducing unnecessary parameters in the model\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 6 :\n",
    "\"\"\"Lasso regression is used for eliminating automated variables and the selection of features.\n",
    "This method is usually used in machine learning for the selection of the subset of variables.\n",
    "It provides greater prediction accuracy as compared to other regression models.\n",
    "Lasso Regularization helps to increase model interpretation.\n",
    "The less important features of a dataset are penalized by the lasso regression.\n",
    "Ridge regression is faster compared to lasso but then again lasso has the advantage\n",
    "of completely reducing unnecessary parameters in the model\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df9f1189-9fb7-4704-a995-c17ba0f0ce1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOverfitting is an occurrence that impacts the performance of a model negatively.\\n\\nIt occurs when a function fits a limited set of data points too closely.\\nData often has some elements of random noise within it.\\n\\nFor example, the training data may contain data points that do not accurately represent the properties of the data.\\nThese points are considered as noise.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 7 :\n",
    "'''\n",
    "Overfitting is an occurrence that impacts the performance of a model negatively.\n",
    "\n",
    "It occurs when a function fits a limited set of data points too closely.\n",
    "Data often has some elements of random noise within it.\n",
    "\n",
    "For example, the training data may contain data points that do not accurately represent the properties of the data.\n",
    "These points are considered as noise.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ee50117-5e06-4707-b313-3f73b5d741d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regularization leads to dimensionality reduction, which means the machine learning model is built using \\na lower dimensional dataset. This generally leads to a high bias errror.\\nIf regularization is performed before training the model, a perfect balance between bias-variance tradeoff must be used.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 8 :\n",
    "\"\"\"Regularization leads to dimensionality reduction, which means the machine learning model is built using \n",
    "a lower dimensional dataset. This generally leads to a high bias errror.\n",
    "If regularization is performed before training the model, a perfect balance between bias-variance tradeoff must be used.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d5a114e-267c-49f7-900f-547ef333d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q No. 9 :\n",
    "#i will choose RMSE as the error has been squared so i will choose RMSE over MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5475547-5277-4869-b145-11c2e4c412c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if i have to reduce overfitting i will use Ridge Regularisation and if i have remove features which \\nare least correlated then i will use Lasso Regularisation.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 10 :\n",
    "\"\"\"if i have to reduce overfitting i will use Ridge Regularisation and if i have remove features which \n",
    "are least correlated then i will use Lasso Regularisation.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
